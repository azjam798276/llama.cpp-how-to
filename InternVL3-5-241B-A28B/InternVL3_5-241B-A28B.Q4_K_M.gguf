./llama-server --model /InternVL3-5-241B-A28B/InternVL3_5-241B-A28B.Q4_K_M.gguf --threads -1 --ctx-size 16384 --n-gpu-layers 99 --device CUDA0,Vulkan0,Vulkan1 --tensor-split 1,0,0 -ot "blk.*.attn_.*=CUDA0" -ot "vision_model.encoder.*=Vulkan0" -ot "blk.([0-9]|[1-4][0-9]|5[0-4])\.ffn_gate_exps.*=Vulkan0" -ot "blk.(5[5-9]|[6-8][0-9]|9[0-3])\.ffn_gate_exps.*=Vulkan1" -ot "blk.*\.ffn_(up|down)_exps.*=CPU" --temp 0.7 --top-p 0.9 --host 0.0.0.0 --port 8080
