./llama-server \
  --model /InternVL3-5-241B-A28B/InternVL3_5-241B-A28B.Q4_K_M.gguf \
  --threads -1 \
  --ctx-size 16384 \
  --n-gpu-layers 99 \
  --device CUDA0,Vulkan0,Vulkan1 \
  --tensor-split 10,24,20 \
  \
  # --- Vision Model ---
  -ot "vision_model.encoder.*=Vulkan0" \
  \
  # --- Attention Layers (attn_.*) Distribution ---
  # First 10 layers on the fastest GPU
  -ot "blk.[0-9].*attn_.*=CUDA0" \
  # Layers 10-59 on the largest GPU
  -ot "blk.(1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]).*attn_.*=Vulkan0" \
  # Layers 60-93 on the second Vulkan GPU
  -ot "blk.(6[0-9]|7[0-9]|8[0-9]|9[0-3]).*attn_.*=Vulkan1" \
  \
  # --- FFN/MoE Layers (ffn_.*) HYBRID Distribution ---
  # Layers 0-39 FFNs on the largest GPU
  -ot "blk.[0-9]|1[0-9]|2[0-9]|3[0-9].*ffn_.*=Vulkan0" \
  # Layers 40-79 FFNs on the second Vulkan GPU
  -ot "blk.(4[0-9]|5[0-9]|6[0-9]|7[0-9]).*ffn_.*=Vulkan1" \
  # FINAL layers (80-93) FFNs on CPU to save VRAM
  -ot "blk.(8[0-9]|9[0-3]).*ffn_.*=CPU" \
  \
  # --- Inference Parameters ---
  --temp 0.7 \
  --top-p 0.9 \
  --host 0.0.0.0 \
  --port 8080
