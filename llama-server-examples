./llama-server \
 --model /app/models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
 --threads -1 \
 --ctx-size 16384 \
 --n-gpu-layers 99 \
 -dev CUDA0,Vulkan0,Vulkan1 \
 --tensor-split 1,0,0 \
 -ot "blk.[0-9].*attn_.*=CUDA0" \
 -ot "blk.[0-9].ffn_.*_exps.=Vulkan0" \
 -ot "blk.1[0-7].ffn_.*_exps.=Vulkan1" \
 -ot "blk.1[0-7].*attn_.*=Vulkan1" \
 -ot "blk.1[8-9]|2[0-9]|3[0-6].*exps.=CPU" \
 --temp 0.7 \
 --top-p 0.9 \
 --host 0.0.0.0 \
 --port 8080


./llama-server -m /app/models/ggml-org/Qwen3-4B-GGUF/Qwen3-4B-Q4_K_M.gguf -dev CUDA0 -c 4096 -ngl 999 -t 10 -b 512 --host 0.0.0.0 --port 8080

vs 

./llama-server \
  --model /app/models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
  --threads -1 \
  --ctx-size 16384 \
  --n-gpu-layers 99 \
  -dev CUDA0,Vulkan0,Vulkan1 \
  --tensor-split 1,0,0 \
  \
  -ot "blk.[0-9].*attn_.*=CUDA0" \
  -ot "blk.[0-9].ffn_.*_exps.=Vulkan0" \
  \
  -ot "blk.1[0-7].ffn_.*_exps.=Vulkan1" \
  -ot "blk.1[0-7].*attn_.*=Vulkan1" \
  \
  -ot "blk.1[8-9].*exps.=Vulkan0" \
  -ot "blk.2[0-1].*exps.=Vulkan1" \
  -ot "blk.(2[2-9]|3[0-6]).ffn_(gate|up|down)_exps.=CPU" \
  \
  --temp 0.7 \
  --top-p 0.9 \
  --host 0.0.0.0 \
  --port 8080

vs
{best config } 
./llama-server \
  --model /app/models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
  --threads -1 \
  --ctx-size 16384 \
  --n-gpu-layers 99 \
  -dev CUDA0,Vulkan0,Vulkan1 \
  --tensor-split 1,0,0 \
  -ot "blk.[0-9].*attn_.*=CUDA0" \
  -ot "blk.[0-9].ffn_.*_exps.=Vulkan0" \
  -ot "blk.1[0-7].ffn_.*_exps.=Vulkan1" \
  -ot "blk.1[0-7].*attn_.*=Vulkan1" \
  -ot "blk.1[8-9].*exps.=Vulkan0" \
  -ot "blk.2[0-1].*exps.=Vulkan1" \
  -ot "blk.(2[2-9]|3[0-6]).ffn_(gate|up|down)_exps.=CPU" \
  --temp 0.7 \
  --top-p 0.9 \
  --host 0.0.0.0 \
  --port 8080
